[["index.html", "My Bookdown My Introduction", " My Bookdown Spencer Zigler Soliz 2024-12-13 My Introduction This is my example of a bookdown document. I apologize for the poor quality and submitting this so late. Thank you for letting me submit this past the due date. "],["api-calls-functions-and-iterations.html", "Chapter 1 API Calls, Functions, and Iterations 1.1 APIs 1.2 NPS Visitation Data 1.3 Functions 1.4 Functionize API Pulls 1.5 Function Defaults 1.6 Iterations 1.7 For loops 1.8 Mapping", " Chapter 1 API Calls, Functions, and Iterations 1.0.1 Lesson Objectives In this lesson we will download data using an application programming interface (API), create our own functions, and iterate using for loops and map(). To fulfill these objectives we will be utilizing two park visitation data sets from the National Park Service (NPS): NPS-wide visitation data, and park unit-specific visitation data. There are seven exercises in this lesson that must be completed. 1.1 APIs An API is software that acts as an intermediary between an online data warehouse (or server) and its users (or clients). As data scientists, APIs provide us a way to request clean and nicely-formatted data that the server will then send to our local computers, all within our RStudio console! To work with APIs, we will need to use two new packages: {httr}, which allows us to communicate with the API’s server, and {jsonlite}, which allows us to work with one of the most common API data formats, JSON. Let’s go ahead and load in our packages for this lesson: library(tidyverse) library(httr) # new - need to install! library(jsonlite) # new - need to install! 1.2 NPS Visitation Data This week, we will be exploring NPS visitor use data across the NPS system as a whole, and and across specific park units. Like many institutions, NPS has a server that stores all of this information (as well as many other things), and an API for users to be able to access it. To utilize the NPS API in R, we first need to explore its API’s data structure. In almost every case, we use URLs to access specific data from APIs. To find the access URL for NPS visitation data, go to Stats Rest API - Documentation (though not very intuitive, the NPS API calls its visitation data set “Stats”). Listed there you will see that all data associated with the “Stats” data set can be accessed using the base URL https://irmaservices.nps.gov/v3/rest/stats. From there, you can tack on additional html text to access two different data sets: total/{year} and visitation. For starters, let’s try accessing the total/{year}. This data set gives us total monthly visitation across all NPS park units, for a user-selected year: https://irmaservices.nps.gov/v3/rest/stats/total/{YEAR} If you tried accessing that URL, you’ll have noticed it doesn’t take you anywhere. This is because the curly brackets {} signify locations in the URL that need to be updated by the user based on their specific needs. I’m curious about visitor use in my birth year, so let’s tweak the URL to access visitation data from 1992. In R, we can access this data using {httr}’s GET() function, replacing {YEAR} with 1992. raw_data &lt;- httr::GET(url = &quot;https://irmaservices.nps.gov/v3/rest/stats/total/1992&quot;) glimpse(raw_data) ## List of 10 ## $ url : chr &quot;https://irmaservices.nps.gov/v3/rest/stats/total/1992&quot; ## $ status_code: int 200 ## $ headers :List of 11 ## ..$ date : chr &quot;Fri, 13 Dec 2024 22:08:33 GMT&quot; ## ..$ content-type : chr &quot;application/json; charset=utf-8&quot; ## ..$ content-length : chr &quot;1420&quot; ## ..$ connection : chr &quot;keep-alive&quot; ## ..$ cache-control : chr &quot;private&quot; ## ..$ server : chr &quot;Microsoft-IIS/10.0&quot; ## ..$ set-cookie : chr &quot;ASP.NET_SessionID=mx55qxsikfqjiwn0daiizcc1; path=/; secure; HttpOnly; SameSite=Lax&quot; ## ..$ x-aspnet-version : chr &quot;4.0.30319&quot; ## ..$ frame-ancestors : chr &quot;*.nps.gov&quot; ## ..$ strict-transport-security: chr &quot;max-age=31536000; includeSubDomains&quot; ## ..$ x-frame-options : chr &quot;SAMEORIGIN&quot; ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;insensitive&quot; &quot;list&quot; ## $ all_headers:List of 1 ## ..$ :List of 3 ## .. ..$ status : int 200 ## .. ..$ version: chr &quot;HTTP/1.1&quot; ## .. ..$ headers:List of 11 ## .. .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;insensitive&quot; &quot;list&quot; ## $ cookies :&#39;data.frame&#39;: 1 obs. of 7 variables: ## ..$ domain : chr &quot;#HttpOnly_irmaservices.nps.gov&quot; ## ..$ flag : logi FALSE ## ..$ path : chr &quot;/&quot; ## ..$ secure : logi TRUE ## ..$ expiration: POSIXct[1:1], format: &quot;Inf&quot; ## ..$ name : chr &quot;ASP.NET_SessionID&quot; ## ..$ value : chr &quot;mx55qxsikfqjiwn0daiizcc1&quot; ## $ content : raw [1:1420] 5b 7b 22 4d ... ## $ date : POSIXct[1:1], format: &quot;2024-12-13 22:08:33&quot; ## $ times : Named num [1:6] 0 0.0174 0.0721 0.1374 0.2417 ... ## ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;redirect&quot; &quot;namelookup&quot; &quot;connect&quot; &quot;pretransfer&quot; ... ## $ request :List of 7 ## ..$ method : chr &quot;GET&quot; ## ..$ url : chr &quot;https://irmaservices.nps.gov/v3/rest/stats/total/1992&quot; ## ..$ headers : Named chr &quot;application/json, text/xml, application/xml, */*&quot; ## .. ..- attr(*, &quot;names&quot;)= chr &quot;Accept&quot; ## ..$ fields : NULL ## ..$ options :List of 2 ## .. ..$ useragent: chr &quot;libcurl/8.3.0 r-curl/5.2.3 httr/1.4.7&quot; ## .. ..$ httpget : logi TRUE ## ..$ auth_token: NULL ## ..$ output : list() ## .. ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;write_memory&quot; &quot;write_function&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;request&quot; ## $ handle :Class &#39;curl_handle&#39; &lt;externalptr&gt; ## - attr(*, &quot;class&quot;)= chr &quot;response&quot; Viewing the data set as-is, you can see it is not super human-readable. This is because data sent from APIs is typically packaged using JavaScript Object Notation (JSON). To unpack the data, we will first need to use {httr}’s content() function. In this example, we want the data to be extracted as text, since this is a data table. Moreover, its encoding is listed as UTF-8. The encoding parameter can be found by opening our raw data set in our R console: raw_data # lists &#39;UTF-8&#39; ## Response [https://irmaservices.nps.gov/v3/rest/stats/total/1992] ## Date: 2024-12-13 22:08 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 1.42 kB # convert content to text unpacked_data &lt;- httr::content(raw_data, as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) Second, we need to transform this string of text, which is still in JSON formatting, into a data frame using {jsonlite}’s fromJSON(): # parse text from JSON to data frame final_data &lt;- jsonlite::fromJSON(unpacked_data) final_data ## Month NonRecreationVisitors RecreationVisitors UnitCode UnitName Year ## 1 1 6209285 10940618 NA NA 1992 ## 2 2 6010027 11931340 NA NA 1992 ## 3 3 6756902 15369139 NA NA 1992 ## 4 4 7255782 21458739 NA NA 1992 ## 5 5 7690763 26648530 NA NA 1992 ## 6 6 7593227 33284625 NA NA 1992 ## 7 7 8438755 41099305 NA NA 1992 ## 8 8 8056823 38625804 NA NA 1992 ## 9 9 7329755 26438266 NA NA 1992 ## 10 10 7105574 23616057 NA NA 1992 ## 11 11 6507805 14338165 NA NA 1992 ## 12 12 6702698 10943961 NA NA 1992 Hooray, you have now successfully pulled in an online data set using an API!  (… also note, the UnitCode and UnitName columns are empty - this is because this is parkwide data and there is no unit code or unit name associated with the entire NPS system. Just a weird NPS data structure thing!) 1.2.1 Exercise #1 Using the code above as a starting point, pull in monthly NPS-wide visitation data for the years 1980, 1999, and 2018. nps_data_1980 &lt;- httr::GET(url = &quot;https://irmaservices.nps.gov/v3/rest/stats/total/1980&quot;) %&gt;% httr::content(as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% jsonlite::fromJSON() nps_data_1999 &lt;- httr::GET(url = &quot;https://irmaservices.nps.gov/v3/rest/stats/total/1999&quot;) %&gt;% httr::content(as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% jsonlite::fromJSON() nps_data_2018 &lt;- httr::GET(url = &quot;https://irmaservices.nps.gov/v3/rest/stats/total/2018&quot;) %&gt;% httr::content(as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) %&gt;% jsonlite::fromJSON() 1.2.2 Exercise #2 Now, let’s explore the second NPS visitation data set, visitation. This call pulls in monthly data for a specific park, across a specific time frame. Use your new API skills to pull in visitation data for Rocky Mountain National Park from 2010 through 2021, based on the API’s URL template. The unit code for Rocky Mountain National Park is ROMO. (Hint: an API URL can have multiple sections that need to be updated by the user; this one requires the starting month and year, the ending month and year, and the park unit of interest.) rmnp_data &lt;- httr::GET( &#39;https://irmaservices.nps.gov/v3/rest/stats/visitation?unitCodes=ROMO&amp;startMonth=1&amp;startYear=2010&amp;endMonth=12&amp;endYear=2021&#39; ) %&gt;% httr::content(as = &#39;text&#39;, encoding = &#39;UTF-8&#39;) %&gt;% jsonlite::fromJSON() 1.3 Functions You may find yourself thinking, “Wow, exercise 1 was overkill!” Indeed, you had to run several lines of code that were nearly identical to what was shown upstream; the only thing you needed to change from one year to the next was the year itself. This sort of redundant coding is not good coding practice. Instead of copying and pasting many coding steps over and over again and tweaking just a tiny portion of it, we can write functions that combine many coding steps into just one command. The benefits of reducing redundant code in this way are threefold. As Grolemund &amp; Wickham describe in their book, R for Data Science: It’s easier to see the intent of your code, because your eyes are drawn to what’s different, not what stays the same. It’s easier to respond to changes in requirements. As your needs change, you only need to make changes in one place, rather than remembering to change every place that you copied-and-pasted the code. You’re likely to have fewer bugs because each line of code is used in more places. Functions provide the option of changing just a minor part of the code base from one run to the next. Think of the GET() function in {httr}: it is a function that has code under-the-hood so that it isn’t necessary to write out the raw code each time we use it. Instead, we call out the function’s name (GET()), and the necessary argument within that function that tweaks the code to fit it to our needs (url = \"&lt;SOME_URL_WE_CHOOSE&gt;\"). 1.4 Functionize API Pulls Let’s try making a function called parkwide_visitation() that pulls in NPS-wide visitation data for a year of choice. To develop a function requires specific formatting: &lt;NAME&gt; &lt;- function(&lt;ARGUMENTS&gt;){ &lt;ACTIONS&gt; return(&lt;OUTPUT&gt;) } … where NAME is what we want to name the function; ARGUMENTS are the variables in the code that get “tweaked”; ACTIONS are the lines of code we want the function to perform (which includes our ARGUMENTS); and the OUTPUT is the object we want as the final outcome of running the function. For parkwide_visitation(), we will use our upstream code as the basis for our function, but with a few minor yet extremely important tweaks: parkwide_visitation &lt;- function(year){ # pull in the data raw_data &lt;- httr::GET(url = # parse out year so that it can be chosen with the &quot;year&quot; # argument, using paste0() paste0(&quot;https://irmaservices.nps.gov/v3/rest/stats/total/&quot;, year)) # convert content to text extracted_data &lt;- httr::content(raw_data, as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) # parse text from JSON to data frame final_data &lt;- jsonlite::fromJSON(extracted_data) return(final_data) } In the above function, our first object, raw_data, now changes based on how we define our year argument. We accomplish this through paste0(), which takes listed objects, transforms them into characters (if they aren’t already), and concatenates them into a single character string. For example: my_sentence &lt;- &quot;I need at least&quot; my_other_sentence &lt;- &quot;pints of ice cream a day&quot; paste0(my_sentence, &quot; &quot;, 08, &quot; &quot;, my_other_sentence, &quot;!&quot;) ## [1] &quot;I need at least 8 pints of ice cream a day!&quot; So, if we make year = 2021 in our parkwide_visitation() function, the year object becomes the number 2021, which makes the paste0() output “https://irmaservices.nps.gov/v3/rest/stats/total/2021”, which subsequently pulls data for 2021. In other words, we can now pull visitation data for any year with just one line of code! pull_2018 &lt;- parkwide_visitation(year = 2018) pull_1980 &lt;- parkwide_visitation(year = 1980) pull_1992 &lt;- parkwide_visitation(year = 1992) # ... and so on! 1.4.1 Exercise #3 Create a function called unit_visitation() that pulls park-specific visitation data for any park, across any time frame. For a list of all park codes for testing the function, download this spreadsheet. (Hint 1: functions can have multiple arguments. For this step, you will want arguments representing the start and end month and year, and park unit). Hint 2: Exercise 2 should be used as a starting point for making this function.) unit_visitation &lt;- function(park_unit, start_month, start_year, end_month, end_year) { raw_data &lt;- httr::GET( url = paste0( &#39;https://irmaservices.nps.gov/v3/rest/stats/visitation?unitCodes=&#39;, park_unit, &#39;&amp;startMonth=&#39;, start_month, &#39;&amp;startYear=&#39;, start_year, &#39;&amp;endMonth=&#39;, end_month, &#39;&amp;endYear=&#39;, end_year ) ) %&gt;% httr::content(as = &#39;text&#39;, encoding = &#39;UTF-8&#39;) %&gt;% jsonlite::fromJSON() } 1.4.2 Exercise #4 Using unit_visitation(), pull in visitation data for Rocky Mountain National Park (ROMO), Everglades National Park (EVER), and Theodore Roosevelt National Park (THRO) from November 1992 through December 2023. pull_ROMO &lt;- unit_visitation(park_unit = &#39;ROMO&#39;, start_month = 11, start_year = 1992, end_month = 12, end_year = 2023) pull_EVER &lt;- unit_visitation(park_unit = &#39;EVER&#39;, start_month = 11, start_year = 1992, end_month = 12, end_year = 2023) pull_THRO &lt;- unit_visitation(park_unit = &#39;THRO&#39;, start_month = 11, start_year = 1992, end_month = 12, end_year = 2023) 1.5 Function Defaults Look at the code that you just wrote; writing out all of those unchanging date arguments still feels repetitive, right? When developing functions, there is an option for setting default values for arguments so that you don’t necessarily have to write all of them out every time you run it in the future. But, the option still exists within the function to make changes when necessary. For example, let’s tweak our parkwide_visitaion() function to have the default year be 2023: parkwide_visitation &lt;- function(year = &quot;2023&quot;) { raw_data &lt;- httr::GET(url = paste0(&quot;https://irmaservices.nps.gov/v3/rest/stats/total/&quot;, year)) # convert content to text extracted_data &lt;- httr::content(raw_data, as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) # parse text from JSON to data frame final_data &lt;- jsonlite::fromJSON(extracted_data) return(final_data) } parkwide_visitation() ## Month NonRecreationVisitors RecreationVisitors UnitCode UnitName Year ## 1 1 13016720 13636466 NA NA 2023 ## 2 2 12518074 14895900 NA NA 2023 ## 3 3 14388294 22215789 NA NA 2023 ## 4 4 14203912 26383797 NA NA 2023 ## 5 5 14928384 31859672 NA NA 2023 ## 6 6 14942656 38753491 NA NA 2023 ## 7 7 15466721 42463634 NA NA 2023 ## 8 8 15097240 37500130 NA NA 2023 ## 9 9 14424743 32418448 NA NA 2023 ## 10 10 14890795 29636427 NA NA 2023 ## 11 11 14026785 19441371 NA NA 2023 ## 12 12 13495629 16293521 NA NA 2023 Because the default year is 2023, you don’t have to write it out explicitly in the function (so long as that’s the year you’re interested in). But, you still have the option of changing the year to something else: parkwide_visitation(year = &quot;1992&quot;) ## Month NonRecreationVisitors RecreationVisitors UnitCode UnitName Year ## 1 1 6209285 10940618 NA NA 1992 ## 2 2 6010027 11931340 NA NA 1992 ## 3 3 6756902 15369139 NA NA 1992 ## 4 4 7255782 21458739 NA NA 1992 ## 5 5 7690763 26648530 NA NA 1992 ## 6 6 7593227 33284625 NA NA 1992 ## 7 7 8438755 41099305 NA NA 1992 ## 8 8 8056823 38625804 NA NA 1992 ## 9 9 7329755 26438266 NA NA 1992 ## 10 10 7105574 23616057 NA NA 1992 ## 11 11 6507805 14338165 NA NA 1992 ## 12 12 6702698 10943961 NA NA 1992 1.5.1 Exercise #5 For our unit_visitation() function, make the default arguments for the start and end months January and December, respectively. This way, we are automatically pulling in data for an entire year. Then, rerun the updated unit_visitation() function for ROMO, EVER, and THRO for the 1980-2023 time period to make sure it works properly. unit_visitation &lt;- function(park_unit, start_month = 1, start_year, end_month = 12, end_year) { raw_data &lt;- httr::GET( url = paste0( &#39;https://irmaservices.nps.gov/v3/rest/stats/visitation?unitCodes=&#39;, park_unit, &#39;&amp;startMonth=&#39;, start_month, &#39;&amp;startYear=&#39;, start_year, &#39;&amp;endMonth=&#39;, end_month, &#39;&amp;endYear=&#39;, end_year ) ) %&gt;% httr::content(as = &#39;text&#39;, encoding = &#39;UTF-8&#39;) %&gt;% jsonlite::fromJSON() } pull_ROMO &lt;- unit_visitation(park_unit = &#39;ROMO&#39;, start_year = 1980, end_year = 2023) pull_EVER &lt;- unit_visitation(park_unit = &#39;EVER&#39;, start_year = 1980, end_year = 2023) pull_THRO &lt;- unit_visitation(park_unit = &#39;THRO&#39;, start_year = 1980, end_year = 2023) 1.6 Iterations At this point, we now know how to develop functions so that we do not have to keep writing out redundant steps in a workflow. However, in that last exercise, you can see that we are still writing out redundant code; we are performing the exact same function on each of our three park units. Another tool for reducing redundancy is iteration, which allows you to do the same thing on multiple inputs. Iteration can happen across different objects, different rows, different data frames, the list goes on and on! 1.7 For loops A for loop is base R’s iteration tool that executes code across a vector, an array, a list, etc. To save the outcome of each iteration, you must first create a vector to store the outputs in that is sized based on how many objects you want to iterate over. For example, I want to run our parkwide_visitation() function over the last five years: 2019, 2020, 2021, 2022, and 2023. To do that, I will first need to develop a vector listing each year: years &lt;- c(&#39;2019&#39;, &#39;2020&#39;, &#39;2021&#39;, &#39;2022&#39;, &#39;2023&#39;) … and then develop an empty list to store each year’s parkwide_visitation() results (i.e., output) into: output_floop &lt;- vector(&quot;list&quot;, length = length(years)) Now that we have a place to store each year’s function results, we can move forward with the for loop itself: for(i in 1:length(years)){ output_floop[[i]] &lt;- parkwide_visitation(year = years[i]) } … where years[i] tells the for loop to perform parkwide_visitation() on the ith year (think of i as the vector position, moving across each year), and output_floop[[i]] directs the for loop to store the results of the ith year’s run into output’s ith list (think of output_floop[[i]] as the location in output_floop that the ith’s results go). We now have a list containing five data frames: one for each year of visitation data: summary(output_floop) ## Length Class Mode ## [1,] 6 data.frame list ## [2,] 6 data.frame list ## [3,] 6 data.frame list ## [4,] 6 data.frame list ## [5,] 6 data.frame list Because each year’s output is structured identically, we can confidently combine each year’s data frame into a single data frame using dplyr::bind_rows(): multi_years &lt;- dplyr::bind_rows(output_floop) 1.7.1 Exercise #6 Use a for loop to run unit_visitation() with arguments start_year = 1980 and end_year = 2021 across ROMO, EVER, and THRO. Then, create a single data frame containing each park units’ output. (Hint: Your first step will be to create a vector listing each park unit.) park_units &lt;- c(&#39;ROMO&#39;, &#39;EVER&#39;, &#39;THRO&#39;) new_output_floop &lt;- vector(&quot;list&quot;, length = length(park_units)) for(i in 1:length(park_units)){ new_output_floop[[i]] &lt;- unit_visitation(park_unit = park_units[i], start_year = 1980, end_year = 2021) } multi_park_units &lt;- dplyr::bind_rows(new_output_floop) 1.8 Mapping The {tidyverse}’s {purrr} package has its own iteration function, map(), that is a variation of the for loop. map() takes a vector and applies a single function across it, then automatically stores all of the results into a list. In other words, map() creates an appropriately sized list to store our results in for us. This eliminates the need to create an empty list ahead of time. To create the same output as our previous for loop on parkwide_visitation(), but using map() instead, we would run the following code: output_map &lt;- years %&gt;% map(~ parkwide_visitation(year = .x)) … where ~ indicates that we want to perform parkwide_visitation() across all years, and .x indicates that we want to use our piped vector, years, as the input to the year argument. As you can see, output_map is identical to output_floop: identical(output_floop, output_map) ## [1] TRUE … which means we can also bind_rows() to get the mapped output into a single data frame: multi_years &lt;- bind_rows(output_map) 1.8.1 Exercise #7 Use map() to run unit_visitation() with arguments start_year = 1980 and end_year = 2021 across ROMO, EVER, and THRO. Then, create a single data frame containing each park units’ output. new_output_map &lt;- park_units %&gt;% map(~ unit_visitation(park_unit = .x, start_year = 1980, end_year = 2021)) identical(new_output_floop, new_output_map) ## [1] TRUE new_multi_park_units &lt;- bind_rows(new_output_map) identical(new_multi_park_units, multi_park_units) ## [1] TRUE "],["data-wrangling-and-visualization.html", "Chapter 2 Data Wrangling and Visualization 2.1 Pulling in necessary packages and data sets 2.2 Exploring our data 2.3 Pivoting 2.4 Joining 2.5 1. Spatial Data Formats 2.6 2. Import and manipulate spatial data 2.7 3. Reading and Writing Spatial Data 2.8 4. Exercises", " Chapter 2 Data Wrangling and Visualization 2.0.1 Lesson Objectives In the last lesson, we learned how to pull data from an API and reduce redundancies in our workflows through functions and iteration. In this lesson we will use the functions in the previous lesson to learn how to manipulate data frames with the {tidyverse}, and plot elegant time series graphs with the {ggplot2}, {scales} and {plotly} packages. There are five exercises in this lesson that must be completed. 2.1 Pulling in necessary packages and data sets library(tidyverse) # ggplot2 is included in the {tidyverse} library(httr) library(jsonlite) library(plotly) # new - need to install! library(scales) # new - need to install! Using the parkwide_visitation() function from the last lesson and mapping, let’s pull park-wide visitor data from 1980-2023, and name the final object parkwide. (Code hack: we can use 1980:2023 to create a vector of years so we don’t have to write each year out!) parkwide_visitation &lt;- function(year){ # pull in the data raw_data &lt;- httr::GET(url = # parse out year so that it can be chosen with the &quot;year&quot; argument, using paste0() paste0(&quot;https://irmaservices.nps.gov/v3/rest/stats/total/&quot;, year)) # convert content to text extracted_data &lt;- httr::content(raw_data, as = &quot;text&quot;, encoding = &quot;UTF-8&quot;) # parse text from JSON to data frame final_data &lt;- jsonlite::fromJSON(extracted_data) return(final_data) } years &lt;- (1980:2021) parkwide &lt;- years %&gt;% map(~ parkwide_visitation(year = .x)) %&gt;% bind_rows() 2.1.1 Exercise #1 Using the unit_visitation() function from the last lesson and mapping, pull visitor data from 1980-2023 for the following park units: ROMO, ACAD, LAKE, YELL, GRCA, ZION, OLYM, and GRSM. Name the final output units. unit_visitation &lt;- function(park_unit, start_month = 1, start_year, end_month = 12, end_year) { raw_data &lt;- httr::GET( url = paste0( &#39;https://irmaservices.nps.gov/v3/rest/stats/visitation?unitCodes=&#39;, park_unit, &#39;&amp;startMonth=&#39;, start_month, &#39;&amp;startYear=&#39;, start_year, &#39;&amp;endMonth=&#39;, end_month, &#39;&amp;endYear=&#39;, end_year ) ) %&gt;% httr::content(as = &#39;text&#39;, encoding = &#39;UTF-8&#39;) %&gt;% jsonlite::fromJSON() } park_units &lt;- c(&#39;ROMO&#39;, &#39;ACAD&#39;, &#39;LAKE&#39;, &#39;YELL&#39;, &#39;GRCA&#39;, &#39;ZION&#39;, &#39;OLYM&#39;, &#39;GRSM&#39;) units &lt;- park_units %&gt;% map(~ unit_visitation(park_unit = .x, start_year = 1980, end_year = 2023)) %&gt;% bind_rows() 2.2 Exploring our data Look at the data frame structure of parkwide and units; they’re exactly the same! So let’s go ahead and bind those together: visitation &lt;- bind_rows(parkwide, units) … except, the rows in parkwide’s UnitCode and UnitCode columns are empty.  Let’s fix the UnitCode column to list “Parkwide” using mutate() and an if_else() statement: visitation &lt;- visitation %&gt;% mutate(UnitCode = if_else(is.na(UnitCode), &quot;Parkwide&quot;, UnitCode)) Think of the above if_else() operation as: “If the column UnitCode is NA, replace NA with”Parkwide“. Otherwise, preserve what is already in the UnitCode column.” Now that we have a single data set containing all of the NPS recreational visitation data that we’ve pulled, let’s start exploring it! But first, let’s aggregate the monthly data into annual data using group_by() and summarize(): annual_visitation &lt;- visitation %&gt;% group_by(UnitCode, Year) %&gt;% # we only care about recreational visitors: summarize(RecVisitation = sum(RecreationVisitors)) annual_visitation ## # A tibble: 394 × 3 ## # Groups: UnitCode [9] ## UnitCode Year RecVisitation ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 ACAD 1980 2779666 ## 2 ACAD 1981 2997972 ## 3 ACAD 1982 3572114 ## 4 ACAD 1983 4124639 ## 5 ACAD 1984 3734763 ## 6 ACAD 1985 3745570 ## 7 ACAD 1986 3929054 ## 8 ACAD 1987 4288154 ## 9 ACAD 1988 4502283 ## 10 ACAD 1989 5440952 ## # ℹ 384 more rows What does visitation data look like through time? First we can try to graph all of the park units together: ggplot(data = annual_visitation)+ geom_point(aes(x = Year, y = RecVisitation, color = UnitCode)) + geom_path(aes(x = Year, y = RecVisitation, color = UnitCode)) + scale_y_continuous(labels = scales::label_scientific()) + theme_bw(base_size = 10) … yikes, not surprisingly, parkwide recreational visitation is wayyyy higher than our individual unit’s visitation data, making our graph pretty useless. It might be nice to have each park unit in a graph of its own. We can create individual graphs for each unit using facet_wrap(), and we can set the y-axes for each plot to \"free_y\": ggplot(data = annual_visitation) + geom_point(aes(x = Year, y = RecVisitation, color = UnitCode)) + geom_path(aes(x = Year, y = RecVisitation, color = UnitCode)) + scale_y_continuous(labels = scales::label_scientific()) + facet_wrap(~UnitCode, scales = &quot;free_y&quot;) + theme_bw(base_size = 10) We can also make this plot interactive by feeding it into {plotly}’s ggplotly() function: plotly::ggplotly( ggplot(data = annual_visitation) + geom_point(aes(x = Year, y = RecVisitation, color = UnitCode)) + geom_path(aes(x = Year, y = RecVisitation, color = UnitCode)) + scale_y_continuous(labels = scales::label_scientific()) + facet_wrap(~UnitCode, scales = &quot;free_y&quot;) + theme_bw(base_size = 10) ) 2.2.1 Exercise #2 Create an interactive graph with two separate panes: one showing park-wide visitation, the other showing all the individual park units together. Both panes should have different y-axes. parkwide_vs_individual &lt;- annual_visitation %&gt;% mutate(Group = ifelse(UnitCode == &quot;Parkwide&quot;, &quot;AllParks&quot;, &quot;IndividualParks&quot;)) plotly::ggplotly( ggplot(data = parkwide_vs_individual) + geom_point(aes(x = Year, y = RecVisitation, color = UnitCode)) + geom_path(aes(x = Year, y = RecVisitation, color = UnitCode)) + scale_y_continuous(labels = scales::label_scientific()) + facet_wrap(~Group, scales = &quot;free_y&quot;) + theme_bw(base_size = 10) ) It is pretty clear that some park units get orders of magnitude more visitors than others. But just how much of the total park visitation do each of these parks account for from year to year? Here we walk through two methods to tackle this question, pivoting and joining, to get park unit visitation side-by-side with park-wide data. 2.3 Pivoting Currently, our annual visitation data is considered long because we have all of our NPS visitation data in one column, with multiple rows representing the same year. We can make this data wide by using the function pivot_wider() wide_data &lt;- annual_visitation %&gt;% select(Year, UnitCode, RecVisitation) %&gt;% pivot_wider(., names_from = UnitCode, values_from = RecVisitation) … where names_from represents the column with the values you are hoping to spread into new columns, and values_from represents the data you want to fill these new columns with. We can make the data set long again by using the function pivot_longer(): long_data &lt;- wide_data %&gt;% pivot_longer(cols = -Year, names_to = &quot;Park&quot;, values_to = &quot;RecVisitation&quot;) … where cols are the columns we want to gather into one column (or, the column(s) you DON’T want to gather), while names_to and values_to are the names and values for the new columns produced from the pivot. 2.3.1 Exercise #3 Using wide_data as the starting point, create an interactive time series plot showing the annual percentage of the total recreational visitation made up by all park units. In other words, a visual that allows us to see how much each park unit contributes to the total NPS system’s recreational visitation. visitation_percentage &lt;- wide_data %&gt;% mutate(across(all_of(park_units), ~ . / Parkwide * 100, .names = &quot;{.col}_Percentage&quot;)) %&gt;% select(contains(&quot;_Percentage&quot;), Year) %&gt;% pivot_longer(cols = contains(&quot;_Percentage&quot;), names_to = &quot;Park&quot;, values_to = &quot;Percentage&quot;) plotly::ggplotly( ggplot(data = visitation_percentage) + geom_point(aes(x = Year, y = Percentage, color = Park)) + geom_path(aes(x = Year, y = Percentage, color = Park)) + theme_bw(base_size = 10)+ labs( title = &quot;Annual Percentage of Total Recreational Visitation&quot;, y = &quot;Percentage of Parkwide Visitation&quot; ) + guides(color = guide_legend(title = &quot;Parks&quot;)) ) 2.4 Joining Another way of getting park-wide visitation side-by-side with the park unit data is through the use of joining our original units and parkwide data sets: joined_data &lt;- inner_join(x = units, # Let&#39;s put &quot;Parkwide&quot; in the UnitCode column so it isn&#39;t empty y = parkwide %&gt;% mutate(UnitCode = &quot;Parkwide&quot;), by = c(&quot;Year&quot;, &quot;Month&quot;)) … where x and y are the two data sets you want joined, and by indicates the column(s) to match them by. When the two data sets you are trying to join have other columns that have the same name, the original column names get “.x” and “.y” appended to them according to their position in the join. Note: there are several ways of joining data. Explore them with ?`mutate-joins` and ?`filter-joins`. 2.4.1 Exercise #4 Using joined_data as the starting point, create an interactive time series plot showing the annual percentage of the total recreational visitation made up by each park unit. This plot should look nearly identical to the previous plot. joined_percentage &lt;- joined_data %&gt;% group_by(Year, UnitCode.x) %&gt;% summarize( RecVisitation_Individual = sum(RecreationVisitors.x), RecVisitation_Parkwide = sum(RecreationVisitors.y), Visitation_Percentage = (RecVisitation_Individual / RecVisitation_Parkwide * 100) ) %&gt;% ungroup() plotly::ggplotly( ggplot(data = joined_percentage, aes( x = Year, y = Visitation_Percentage, color = UnitCode.x )) + geom_point() + geom_path() + theme_bw(base_size = 10) + labs( title = &quot;Annual Percentage of Total Recreational Visitation&quot;, y = &quot;Percentage of Parkwide Visitation&quot; ) + guides(color = guide_legend(title = &quot;Parks&quot;)) ) 2.4.2 Exercise #5 Which park on average has the most recreational visitation? Which park has the least recreational visitation? Base your response on the data starting in 1990, ending in 2023. Defend your answer with numbers! joined_percentage %&gt;% filter(Year &gt;= 1990) %&gt;% group_by(UnitCode.x) %&gt;% summarise(MeanVisitation = mean(Visitation_Percentage)) ## # A tibble: 8 × 2 ## UnitCode.x MeanVisitation ## &lt;chr&gt; &lt;dbl&gt; ## 1 ACAD 0.930 ## 2 GRCA 1.61 ## 3 GRSM 3.50 ## 4 LAKE 2.84 ## 5 OLYM 1.12 ## 6 ROMO 1.14 ## 7 YELL 1.17 ## 8 ZION 1.03 #GRSM has the most recreational visitation and ACAD has the least based off the mean of the visitation percentage starting in 1990 # Intro to Spatial Data in R 2.5 1. Spatial Data Formats Vector Data Locations (points) Coordinates, address, country, city Shapes (lines or polygons) Political boundaries, roads, building footprints, water bodies Raster Data Images (matrix of cells organized by rows and columns) Satellite imagery, climate, landcover, elevation 2.6 2. Import and manipulate spatial data There are a few new R packages we will need to work with spatial data, listed below with hyperlinks and decribed in more detail throughout this and other lessons. sf : working with vector data terra : working with raster data tmap : visualizing spatial data (i.e., making maps!) tigris : import vector data from the U.S. Census database (i.e., political boundaries, roads, etc.) elevatr : import elevation data rgbif (optional) : import species occurrence data from the GBIF database soilDB (optional) : import snow depth data from SNOTEL sites across the U.S. We’ve already added these packages to a ‘setup.R’ script in this project directory, so you can use source(\"setup.R\") at the beginning of each lesson if you want, otherwise you will need to install each new one manually with install.packages(). source(&quot;setup.R&quot;) 2.6.1 2.1 Vector Data 2.6.2 tigris 2.6.3 Polygons All the data we are working with in this lesson is confined to the state of Colorado. Let’s start by pulling in political boundaries for Colorado counties with the tigris package, which returns a shapefile consisting of polygons for each county. # download county shapefile for the state of Colorado co_counties &lt;- counties(state = &quot;CO&quot;) The tigris package is one of many data retrieval R packages that uses API calls to pull in data from various online/open databases directly into your R session, without the need to separately download. When you close out your R session, these ‘temp’ files are erased, so it does not use up any of your local storage. At the end of this lesson you will learn how to save shapefiles to your computer if you do in fact want to store and use them in the future (e.g., you manipulated a data set quite a bit and don’t want to re-run the entire process every new R session). 2.6.4 Lines tigris has many other data sets in addition to political boundaries. Today let’s work with another shapefile, importing roads for Larimer county, which returns a polyline dataset for all roads in Larimer County. co_roads &lt;- roads(state = &quot;CO&quot;, county = &quot;Larimer&quot;) 2.6.5 tmap Throughout this lesson we will be using the tmap package to produce quick static or interactive maps. tmap allows for both static (“plot” mode) and interactive (“view” mode) mapping options, which you can set using the function tmap_mode() . For today we will be making quick interactive plots. Once you set the mode with tmap_mode(), every plot call to tmap after that produces a plot in that mode. tmap_mode(&quot;view&quot;) Lets view our Colorado counties and Larimer County roads shapefiles. To make a “quick thematic map” in tmap you can use the qtm() function. You can also use tm_shape() plus the type of spatial layer (e.g., tm_polygons()) to add your layers to the map. Both methods below will produce the same exact map, and you may think why would you ever need to use the tm_shape() method since its more code? The answer may be rarely, but there are some cases where you can customize your maps better with tm_shape() that we will see later on. Also notice that tmap uses + signs to tack on additional maps/elements similar to ggplot2 code (i.e., no pipe!) Note: map rendering may take a few seconds because the roads layer is pretty large and detailed. # Option 1: Using qtm() qtm(co_counties)+ qtm(co_roads) # Option 2: Using tm_shape() tm_shape(co_counties)+ tm_polygons()+ tm_shape(co_roads)+ tm_lines() Mess around with this map a little bit. See that you can change the basemap, turn layers on and off, and click on features to see their attributes. There are a ton of ways to customize these maps (more details on this in the spatial viz lesson!). For example, co_counties has an ‘ALAND’ variable, which represents the total land area of each county. To color by that variable we would use: qtm(co_counties, fill = &quot;ALAND&quot;) Let’s inspect the spatial data sets a little more. What do you see when you run the following line of code? class(co_counties) 2.6.6 sf By default, the tigris package imports spatial data in sf format, which stands for ‘simple features’. The sf package provides an easy and efficient way to work with vector data, and represents spatial features as a data.frame or tibble with a geometry column, and therefore also works well with tidyverse packages to perform manipulations like you would a data frame. For example, we are going to do an exercise for the Poudre Canyon Highway, so we want to filter out the roads data set to only those features. Using your investigative geography skills and your interactive map, find the highway on your map and find out what the exact ‘FULLNAME’ attribute is, and use that to filter() the data set. Call the new roads feature poudre_hwy. poudre_hwy &lt;- co_roads %&gt;% filter(FULLNAME == &quot;Poudre Canyon Hwy&quot;) qtm(poudre_hwy) 2.6.7 Points Most often when you are working with points, you start with an excel file or something similar that consists of the raw latitude and longitude. When you have spatial data that is not explicitly spatial yet or not in the sf format, you use the st_as_sf() function to transform it. Lets work with a couple locations along the Poudre highway, making a small data frame of their coordinates: poudre_points &lt;- data.frame(name = c(&quot;Mishawaka&quot;, &quot;Rustic&quot;, &quot;Blue Lake Trailhead&quot;), long = c(-105.35634, -105.58159, -105.85563), lat = c(40.68752, 40.69687, 40.57960)) Right now, poudre_points is just a data frame (run class(poudre_points) to check). We need to convert it to a spatial (sf) object first in order to map and spatially analyze it. Within the st_as_sf() function we need to specifying the longitude and latitude columns in our poudre_points data frame and the CRS (Coordinate Reference System). Note that ‘x’ (longitude) always goes first followed by ‘y’ (latitude). Otherwise it will map your points on the other side of the world. poudre_points_sf &lt;- st_as_sf(poudre_points, coords = c(&quot;long&quot;, &quot;lat&quot;), crs = 4326) qtm(poudre_hwy)+ qtm(poudre_points_sf) Note the 4-digit number we assign for crs. This is an EPSG code, which is tied to a specific CRS called WGS84 and one of the most common reference systems coordinates are recorded in (often noted by the fact that the values are in decimal degrees). This is used by Google Earth, the U.S. Department of Defense and all GPS satellites (among others). A full list of EPSG codes and coordinate reference systems can be found here. Note, there are A LOT. Probably the most common used in the U.S. are WGS84 (a global CRS) and NAD83 (used by many U.S. federal agencies). 2.6.8 Coordinate Reference Systems Probably the most important part of working with spatial data is the coordinate reference system (CRS) that is used. The CRS describes how and where your spatial data is located on Earth. There are numerous different CRS’s depending on when and how the data was collected, the spatial location and extent it was collected, etc. In order to analyze and visualize spatial data, all objects must be in the exact same CRS. We can check a spatial object’s CRS by printing it the object name to the console, which will return a bunch of metadata about the object. You can specifically return the CRS for sf objects with st_crs(). # see the CRS in the header metadata: co_counties #return just the CRS (more detailed) st_crs(co_counties) You can check if two objects have the same CRS like this: st_crs(poudre_hwy) == st_crs(poudre_points_sf) Uh oh, the CRS of our points and lines doesn’t match. While tmap performs some on-the-fly transformations to map the two layers together, in order to do any analyses with these objects you’ll need to re-project one of them. You can project one object’s CRS to that of another with st_transform like this: # transform the CRS of poudre_points_sf to the CRS of poudre_hwy poudre_points_prj &lt;- st_transform(poudre_points_sf, st_crs(poudre_hwy)) # Now check that they match st_crs(poudre_points_prj) == st_crs(poudre_hwy) 2.6.9 2.2 Raster Data 2.6.10 elevatr Lets import some elevation data using the elevatr package. The function get_elev_raster() returns a raster digital elevation model (DEM) from the AWS Open Data Terrain Tiles. For this function you must supply a spatial object specifying the extent of the returned elevation raster and the resolution (specified by the zoom level z). We are importing elevation at ~ 1km resolution (more like 900 m), and we can use our co_counties object as the extent we want to download to, which will return elevation tiles for the state of Colorado. Note: ‘extent’ is the spatial bounding box of the data (represented by the x,y coordinates of the four corners inclusive of the entire spatial data) co_elevation &lt;- get_elev_raster(co_counties, z = 7) qtm(co_elevation) By default, tmap uses a categorical symbology to color the cells by elevation. You can change that to a continuous palette like this (an example of when tm_shape() allows us to edit the map more): tm_shape(co_elevation)+ tm_raster(style = &quot;cont&quot;, title = &quot;Elevation (m)&quot;) When we see this on a map, we see that it actually extends beyond Colorado due to how the Terrain Tiles are spatially organized. Let’s inspect this raster layer a little. By printing the object name to the console we see a bunch of metadata like resolution (cell/pixel size), extent, CRS, and file name. co_elevation 2.6.11 terra We use the terra package to work with raster data. For example, we only want to see elevation along the Poudre highway. We can use crop to crop the raster to the extent of our poudre_hwy spatial object using the ext() function to get the extent (i.e., bounding box) of our poudre_hwy object. However…the following line of code doesn’t work: # If we try this, we get an error co_elevation_crop &lt;- crop(co_elevation, ext(poudre_hwy)) This doesn’t work because our co_elevation object is actually not in the proper format to work with the terra package. The elevatr package still uses the raster package to work with raster data, however this package is outdated and we want to stick with terra for this course and any future work you do with raster data. # note the data type of elevation is RasterLayer class(co_elevation) terra uses objects of a new class called SpatRaster. Converting a RasterLayer to a SpatRaster is quick using the rast() function. co_elevation &lt;- rast(co_elevation) Now check the class: class(co_elevation) Now we can use terra functions, and re-run the crop() code we tried earlier: co_elevation_crop &lt;- crop(co_elevation, ext(poudre_hwy)) Plot all our spatial layers together: qtm(co_elevation_crop) + qtm(poudre_hwy) + qtm(poudre_points_prj) 2.7 3. Reading and Writing Spatial Data 2.7.1 3.1 Writing spatial data All of the spatial data we’ve worked with are only saved as objects in our environment. To save the data to disk, the sf and terra packages have functions to do so. You are not required to save these files, but if you want to follow along with these functions save the data to the ‘data/’ folder. To save vector data with sf, use write_sf() write_sf(poudre_hwy, &quot;data/poudre_hwy.shp&quot;) write_sf(poudre_points_prj, &quot;data/poudre_points.shp&quot;) While you can give the file any name you want, note that you must put ‘.shp’ as the extension of the file. While ‘.shp’ stands for ‘shapefile’, if you run the code above you’ll notice a bunch of other files are saved, having the same file name but different extensions. These are auxiliary files required to properly work with the .shp shapefile. If you ever want to share or move a shapefile, you must zip all the auxiliary files and .shp file together. Think of them as a package deal! To save raster data with terra use writeRaster() writeRaster(co_elevation_crop, &quot;data/poudre_elevation.tif&quot;) Same as with the vector data, when saving raster data you must add the ‘.tif’ file extension to the name. There are various formats raster data can be stored as (e.g., ASCII, ESRI Grid) but GeoTiffs are the most common and generally easiest to deal with in R. 2.7.2 3.2 .RData Files Another way you can store data is saving your environmental variables as R Data objects. You may have already seen ‘.RData’ files in your folders before if you ever click ‘yes’ when closing out of RStudio asks you to save your workspace. What this does is save everything in your environment to a file with a ‘.RData’ extension in your project directory, and then every time you open your project it reloads everything that was in the environment. This however is often poor practice, as it prevents you from writing reproducible code and all those variables start racking up storage space on your computer. We recommend changing this setting by going to Global Options and under ‘Workspace’ set ‘Save workspace to .RData on exit’ to ‘Never’. However, there are times you may want to save your variables as R files, such as when you have a set of variables you want to quickly re-load at the beginning of your session, or some files that are pretty large in size which is often the case with spatial data (R object files are much smaller). You can save single or multiple variables to an .RData file, or single variables to an .RDS file. Since the poudre_hwy and poudre_points_prj were objects you created in this session, to avoid the need to recreate them you can save them to an .RData file with save() : save(poudre_hwy, poudre_points_prj, file = &quot;data/poudre_spatial_objects.RData&quot;) Note that you must add the ‘file =’ to your second argument. Now to test out how .RData files work, remove them from your environment with rm() (be careful with this function though, it is permanent!) and load them back in with load() rm(poudre_hwy, poudre_points_prj) See they are no longer in your Environment pane, but after you load the .RData file back in, it loads in those two objects with the same environmental names they were given when you saved them. load(&quot;data/poudre_spatial_objects.RData&quot;) Note that terra objects don’t properly save to .RData files, but there is a work around if you save a single terra object as an .RDS file with saveRDS. Here is that workflow, there is just a second step to ‘unpack’ the loaded .RDS object with rast(). saveRDS(co_elevation_crop, &quot;data/poudre_elevation.RDS&quot;) readRDS(&quot;data/poudre_elevation.RDS&quot;) %&gt;% rast() Note that with .RDS files you must assign the loaded file to a new environmental variable (unlike with .RData that returns the objects with the exact names they had before). 2.7.3 3.3 Reading Spatial Data To read in shapefiles, you use read_sf() . If you saved the poudre_hwy shapefile in the steps above, you can load it back into your environment like this: read_sf(&quot;data/poudre_hwy.shp&quot;) Notice that when reading shapefiles into R you only specify the file with the ‘.shp’ extension, and don’t need to pay much attention to any of those auxiliary files. As long as all the other auxiliary files are saved in that same folder, it will read in the shapefile correctly, but if you are missing any then the .shp file becomes useless on its own. To read in raster files you use the rast() function and file path with the appropriate file extension rast(&quot;data/poudre_elevation.tif&quot;) Remember when reading in files you will want to assign them to a new variable name with &lt;- to keep them in your environment. 2.8 4. Exercises Explore the use of extract from the terra package by running ?terra::extract. (Note we need to specify terra:: because ‘extract’ is a function name in multiple packages we may have loaded in our session). How would you extract the elevation at each of the three points in poudre_points_prj ? (2 pts) poudre_elevation &lt;- rast(&quot;data/poudre_elevation.tif&quot;) terra::extract(poudre_elevation, poudre_points_prj, method = &quot;simple&quot;) Choose your favorite state (other than Colorado). For that state, carry out the following tasks: (8 pts) Import the county boundaries for your state: wa_counties &lt;- counties(state = &quot;WA&quot;) qtm(wa_counties) Import elevation for your state (using your new counties object as the extent/bounding box and set z = 7): wa_elevation &lt;- get_elev_raster(wa_counties, z = 7) qtm(wa_elevation) Create an interactive map of your state counties and the elevation layer underneath (note: use ?qtm to see the argument options for fill = to draw only the county borders, i.e. remove the fill color). qtm(wa_counties, fill = NULL) + qtm(wa_elevation) Choose a single county within your state county object, and crop your elevation layer to the extent of that county (note: use filter() to create an object of just your selected county that you want to crop to). Follow the steps above we used to crop co_elevation to the poudre hwy. clallam_county &lt;- wa_counties %&gt;% filter(NAMELSAD == &quot;Clallam County&quot;) wa_elevation &lt;- rast(wa_elevation) names(wa_elevation) &lt;- &quot;Elevation&quot; wa_clallam_crop &lt;- crop(wa_elevation, ext(clallam_county)) qtm(wa_clallam_crop) + qtm(clallam_county, fill = NULL) "],["spatial-data-analysis.html", "Chapter 3 Spatial Data Analysis 3.1 Load in spatial data 3.2 Distance Calculations 3.3 Buffers 3.4 Spatial Intersect 3.5 Raster Reclassification 3.6 Extraction Statistics", " Chapter 3 Spatial Data Analysis In the first lesson this week you were introduced to different spatial data types, various databases you can pull spatial data from and worked through importing, wrangling, and saving those spatial data types. Today we are going to dive deeper in spatial analyses in R. You have briefly used the sf and terra packages so far, but today we will be exploring them much more in depth using the wide range of spatial analysis operations they provide. You shouldn’t need to install any new packages for today, but need to load in all the necessary libraries: source(&quot;setup.R&quot;) 3.1 Load in spatial data We will be working with some new datasets today that are already included in the ‘data/’ folder. These include: “spatDat.RData” : an .RData file that loads in the four objects: counties : a multipolygon layer of Colorado counties (which we used in 01_spatial_intro.Rmd) rivers : a polyline layer of all major rivers in Colorado occ : a list of three dataframes that includes species occurrence data (i.e., point locations) for Elk, Yellow-bellied Marmot, and Western Tiger Salamander in Colorado retrieved from the GBIF database. snotel_data : spatial point dataframe (i.e., sf object) of daily snow depth for 8 SNOTEL sites in Colorado #load in all your vector data load(&quot;data/spatDat.RData&quot;) #read in the elevation and landcover rasters landcover &lt;- terra::rast(&quot;data/NLCD_CO.tif&quot;) elevation &lt;- terra::rast(&quot;data/elevation.tif&quot;) 3.1.1 Bonus Lesson All the above objects were retrieved and cleaned in R. The lesson plan in the ‘bonus/’ folder titled ‘get_spatial_challenge.Rmd’ is an assignment that tasks you with importing and cleaning the data that was saved in ‘spatDat.RData’. If you complete this challenge assignment fully you will get up to 3 extra credit points. Even if you don’t want to complete this challenge, it is worth your while to read and work through it! 3.2 Distance Calculations We’re going to start off today with some distance calculations. Using our species occurrence data, say we want to know on average how far away is each species found from a major river, and compare that among species. Throughout today we are going to be mapping our spatial data to quickly inspect it and get a visual of the data’s extent and characteristics, so lets set our tmap mode to interactive. tmap_mode(&quot;view&quot;) First, our occ object is not in a spatial format. We first need to bind our dataframes into a single one, and convert it to an sf object using st_as_sf() : occ_sp &lt;- bind_rows(occ) %&gt;% st_as_sf(coords = c(&quot;decimalLongitude&quot;, &quot;decimalLatitude&quot;), crs = 4236) We set the CRS to 4236, which is the EPSG code for WGS84, the most commonly used CRS for GPS coordinates (But I also checked the GBIF metadata to make sure it was in fact WGS84). Quick view of all our points, colored by species: qtm(occ_sp, symbols.col = &quot;Species&quot;) Now, calculating the distance to the nearest river involves point to line distance calculations, which we can perform with the sf package. Before performing any spatial operations, remember all of our spatial objects must be in the same CRS. 3.2.1 Exercise #1 st_crs(rivers) == st_crs(occ_sp) The CRS of our objects does not match. Using what you learned in week one, conduct a spatial transformation to our occ_sp object to coerce it to the same CRS of our rivers object. Call the new object occ_prj and double check that rivers and our new occurrences object are in the same CRS after transforming occ_prj &lt;- st_transform(occ_sp, st_crs(rivers)) st_crs(rivers) == st_crs(occ_prj) Now lets visualize our rivers and occurrence data: qtm(rivers) + qtm(occ_prj, symbols.col = &quot;Species&quot;) Our occurrence data set covers all of Colorado, but rivers are only for Larimer County. So, we have to first filter our points to Larimer County. Similar to filter() from the {tidyverse}, we can use st_filter() to perform a spatial filtering (i.e., we want to filer just the points that occur in Larimer County). 3.2.2 Exercise #2 Use ?st_filter to explore the use of the function, and then use it to filter our occ_prj points to Larimer county and call the new object occ_larimer. Note: You will first need to create a spatial object of just Larimer county to use as a filter. larimer &lt;- counties %&gt;% filter(NAMELSAD == &quot;Larimer County&quot;) occ_larimer &lt;- st_filter(occ_prj, larimer) qtm(occ_larimer) Great, now we just have species occurrences within Larimer County. Now for each point we want to calculate its distance to the nearest river. The most efficient way is to first find the nearest line feature for each point. We can do this with the st_nearest_feature() function. This function returns the index values (row number) of the river feature in the rivers spatial data frame that is closest in distance to each point. Here we are assigning these index values to a new column of our Larimer occurrences called ‘nearest_river’ that we will use later to calculate distances: occ_larimer$nearest_river &lt;- st_nearest_feature(occ_larimer, rivers) Now, for each point we can use the st_distance() function to calculate the distance to the nearest river feature, using the index value in our new “nearest_river” column. Adding by_element = TRUE is necessary to tell the function to perform the distance calculations by element (row), which we will fill into a new column “river_dist_m”. occ_larimer$river_dist_m &lt;- st_distance(occ_larimer, rivers[occ_larimer$nearest_river, ], by_element = TRUE) Notice that the new column “river_dist_m” is more than just a numeric class, but a “units” class, specifying that the values are in meters. str(occ_larimer) 3.2.3 Exercise #3 Cool, now you have the distance to the nearest river (in meters) for each individual species occurrence, but you want the average distance for each species. Using what you know of the dplyr functions, calculate the species average distance, then make a bar plot to compare the averages among species: Hint: remember that the new distance column is a ‘units’ data type will throw an error when you try to plot those values. You will need to make use of mutate() and as.numeric within your string of operations in order to complete task. sp_avg_dist &lt;- occ_larimer %&gt;% group_by(Species) %&gt;% mutate(across(river_dist_m, as.numeric)) %&gt;% mutate(avg_dist = mean(river_dist_m)) sp_avg_dist %&gt;% ggplot() + geom_col(aes(x = Species, y = avg_dist, fill = Species)) + theme_bw() + labs(y = &quot;Distance&quot;, title = &quot;Average Distance to Nearest River by Species&quot;) Which species is, on average, found closest to a river? The Western Tiger Salamander is found on average closest to a river. 3.3 Buffers Alternatively, say you want to know what percentage of species’ occurrences (points) were found within a specified distance of a river (calculated buffer). Here lets investigate how often each species is found within 100m of a river. To do this we can add a buffer around our line features and filter the points that fall within that buffer zone. We can use st_buffer() with a specified distance (default is meters since our rivers object uses ‘meters’ as its length unit, we can tell by checking the CRS with st_crs()) river_buffer &lt;- st_buffer(rivers, dist = 100) qtm(river_buffer) If you zoom in on the map you can now see a buffer around the rivers, and this new object is actually a polygon geometry type now instead of a line. river_buffer 3.4 Spatial Intersect We can conduct spatial intersect operations using the function st_intersects(). This function checks if each occurrence intersects with the river buffer, and if so it returns an index value (row number) for the river feature it intersects. This function returns a list object for each occurrence, that will be empty if there are no intersections. We will add this as a column to our occurrence data set, and then create a binary yes/no river intersection column based on those results (is the list empty or not?). First look at what st_intersects() returns: st_intersects(occ_larimer, river_buffer) We see it is a list of the same length as our occ_larimer object, where each list element is either empty (no intersections) or the index number for the river buffer feature it intersects with. To add this as a new column in our occ_larimer data we run this: occ_larimer$river_intersections &lt;- st_intersects(occ_larimer, river_buffer) Now we can create a new column in occ_larimer called ‘river_100m’ that returns TRUE/FALSE if the buffer intersects with a river. We make use of if_else() and the lengths() function to check the length of each list element in each row, as the empty ones will return a length of 0. If the length is zero/empty, then we return FALSE meaning that occurrence was not found within 100m of a river. occ_rivers &lt;- occ_larimer %&gt;% mutate(river_100m = if_else(lengths(river_intersections) == 0, FALSE, TRUE)) Now we can calculate what percentage of occurrences are within 100 m of a river for each species using dplyr operations. Which species is most often found within 100m of a river? Elk are most often found within 100m of a river. occ_rivers %&gt;% group_by(Species) %&gt;% summarise(total_occ = n(), total_river = sum(river_100m == TRUE), percent_river = (sum(river_100m == TRUE)/total_occ)*100) 3.4.0.1 Reflection This analysis is just for teaching purposes, why would you be cautious about these results for answering real research questions? Think about how we filtered everything to a political boundary, what’s wrong with this method? Animals aren’t bound to political boundaries. 3.5 Raster Reclassification So far we’ve dealt with a bunch of vector data and associated analyses with the sf package. Now lets work through some raster data analysis using the terra package. First, lets explore the landcover raster by making a quick plot. qtm(landcover) This land cover data set includes attributes (land cover classes) associated with raster values. This is because of the .aux auxiliary file paired with the .tif. in the ‘data/’ folder. Similar to shapefiles, this file provides metadata (in this case land cover class names) to the raster file. We can quickly view the frequency of each land cover type with the freq() function, where ‘count’ is the number of pixels in the raster of that landcover type. freq(landcover) 3.5.1 Exercise 4 Create a bar chart of landcover frequency, and order the bars highest to lowest (see this resource to guide you on sorting bars by a numeric variable/column). Also investigate the use of coor_flip() and how it might make your plot look better… land_freq &lt;- as.data.frame(landcover, xy = TRUE) %&gt;% rename(Class = &quot;NLCD Land Cover Class&quot;) %&gt;% count(Class) ggplot(land_freq, aes(x = reorder(Class, -n), y = n, fill = Class)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + labs(x = &quot;Frequency&quot;, y = &quot;Land Cover Class&quot;, title = &quot;Frequency of Land Cover by Class in Colorado&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Say we want to explore some habitat characteristics of our species of interest, and we are specifically interested in forest cover. We can use raster reclassification to create a new layer of just forest types in Colorado. Since rasters are technically matrices, we can using indexing and change values quickly using matrix operations. Given this particular raster uses character names associated with values (thanks to the .aux file!), we can index by those names. #first assign landcover to a new object name so we can manipulate it while keeping the original forest &lt;- landcover #where the raster equals any of the forest categories, set that value to 1 forest[forest %in% c(&quot;Deciduous Forest&quot;, &quot;Evergreen Forest&quot;, &quot;Mixed Forest&quot;)] &lt;- 1 #SPELLING IS IMPORTANT #now set all non forest pixels to NA forest[forest != 1] &lt;- NA Now plot the new forest layer to get a quick sense if it looks accurate or not. plot(forest) 3.6 Extraction Statistics When we want to summarize raster values for certain shapes (points, polygons, etc), the extract() function from the terra package helps us do that. Say we want to find out the most common land cover type each of our species is found in. We can use extract() to get the landcover value from the raster at each of our occurrence points, and then do some summary statistics. Within this function, the first element is the raster you want to get values from, and the second element is the spatial layer you want to extract values at. Here we will use our landcover raster layer and the occ_prj object to extract values for occurrences across Colorado. First, we need to project our landcover raster to the CRS of our occurrences, otherwise the operation will only return NAs. # project the landcover layer landcover_prj &lt;- project(landcover, crs(occ_prj)) extract(landcover_prj, occ_prj) Notice that this returns a 2 column data frame, with an ID for each feature (occurrence) and the extracted raster value in the second column. We can actually use extract() within mutate() to add the values as a new column to our occurrences data frame so we can do further summary statistics. However, since extract() returns a 2 column data frame, it will nest this into a single column in the occ_prj data frame. To separate this into two separate columns we can use unnest() : occ_landcover &lt;- occ_prj %&gt;% mutate(common_landcover = extract(landcover_prj, occ_prj)) %&gt;% unnest(common_landcover) %&gt;% #lets rename the land cover column which is now called &quot;NLCD Land Cover Class&quot; rename(common_landcover = &quot;NLCD Land Cover Class&quot;) Now, we can find the most common land cover type for each species, using some tidyverse wrangling. Note the use of st_drop_geometry(), this reverts the sf object back to an original data frame, which is required for some tidyverse operations. occ_landcover %&gt;% st_drop_geometry() %&gt;% # this converts the data back to a dataframe, required for some tidyverse operations group_by(Species) %&gt;% count(common_landcover) %&gt;% slice(which.max(n)) #returns the row with the highest count &quot;n&quot; We can also use extract() to extract raster values within polygons, but here must supply some function of how to summarize all the values within each polygon. For this example, lets find the most common landcover type in each Colorado county. county_landcover &lt;- counties %&gt;% mutate(landcover = extract(landcover_prj, counties, fun = &quot;modal&quot;)) %&gt;% unnest(landcover) %&gt;% rename(value = &quot;NLCD Land Cover Class&quot;) #renaming this helps us perform a join later on... Uh oh, this gives us the raw pixel values instead of the land cover classes. We can get a table of value - class pairs by using the cats() function: classes &lt;- as.data.frame(cats(landcover)) #coerce to a data frame because cats() actually returns it as a list Value and NLCD.Land.Cover.Class are our cell value - class pairs. Now we want to join this to our county_landcover object to get the actual land cover name. 3.6.1 Exercise 5 Perform the appropriate *_join operation to tie our county_landcover and classes data frames together. Then make a map of the counties each colored/filled by the most common NLCD land cover class. county_landcover_join &lt;- right_join(classes, county_landcover) county_landcover_join &lt;- st_as_sf(county_landcover_join, coords = c(&quot;INTPTLON&quot;, &quot;INTPTLAT&quot;), crs = &#39;NAD83&#39;) county_landcover_summary &lt;- st_join(counties, county_landcover_join) qtm(county_landcover_summary, fill = &quot;NLCD.Land.Cover.Class&quot;) 3.6.2 Exercise 6 Find the average elevation each species occurs at (for all Colorado occurrences). Which species is, on average, found at the highest elevations? Hints: Use the elevation and occ_prj objects we have created or read in above. Remember to check the CRS and perform a spatial transformation if necessary! All parts needed to answer this question have been introduced in this lesson plan. st_crs(elevation) == st_crs(occ_prj) elevation_prj &lt;- project(elevation, crs(occ_prj)) occ_elevation &lt;- occ_prj %&gt;% mutate(elevation_found = extract(elevation_prj, occ_prj)) %&gt;% unnest(elevation_found) %&gt;% st_drop_geometry() %&gt;% group_by(Species) %&gt;% drop_na() %&gt;% summarise(elevation_avg = mean(Elevation)) print(occ_elevation) Yellow-bellied Marmots are found, on average, at a higher elevation than both Elk and Western Tiger Salamander. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
